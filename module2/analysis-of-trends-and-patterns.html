<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>3 Analysis of trends and patterns | Module 2 Making sense of tabular data</title>
  <meta name="description" content="This is the second module in the FIT5147 Data Exploration and Visualisation unit. In this module you will learn about common graphics for showing tabular data. These are the kind of graphics that are common in statistics.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="3 Analysis of trends and patterns | Module 2 Making sense of tabular data" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the second module in the FIT5147 Data Exploration and Visualisation unit. In this module you will learn about common graphics for showing tabular data. These are the kind of graphics that are common in statistics." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Analysis of trends and patterns | Module 2 Making sense of tabular data" />
  
  <meta name="twitter:description" content="This is the second module in the FIT5147 Data Exploration and Visualisation unit. In this module you will learn about common graphics for showing tabular data. These are the kind of graphics that are common in statistics." />
  

<meta name="author" content="Kimbal Marriott">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="visualising-tabular-data.html">
<link rel="next" href="activity-advanced-plots-with-r.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Module 2 Making sense of tabular data</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Making sense of tabular data: Overview</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#aims-of-this-module"><i class="fa fa-check"></i><b>1.1</b> Aims of this module</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#how-to-study-for-this-module"><i class="fa fa-check"></i><b>1.2</b> How to study for this module</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="visualising-tabular-data.html"><a href="visualising-tabular-data.html"><i class="fa fa-check"></i><b>2</b> Visualising tabular data</a><ul>
<li class="chapter" data-level="2.1" data-path="visualising-tabular-data.html"><a href="visualising-tabular-data.html#basic-statistical-graphics"><i class="fa fa-check"></i><b>2.1</b> Basic statistical graphics</a></li>
<li class="chapter" data-level="2.2" data-path="visualising-tabular-data.html"><a href="visualising-tabular-data.html#multivariate-data"><i class="fa fa-check"></i><b>2.2</b> Multivariate data</a></li>
<li class="chapter" data-level="2.3" data-path="visualising-tabular-data.html"><a href="visualising-tabular-data.html#showing-3d"><i class="fa fa-check"></i><b>2.3</b> Showing 3D</a></li>
<li class="chapter" data-level="2.4" data-path="visualising-tabular-data.html"><a href="visualising-tabular-data.html#additional-visual-attributes"><i class="fa fa-check"></i><b>2.4</b> Additional visual attributes</a></li>
<li class="chapter" data-level="2.5" data-path="visualising-tabular-data.html"><a href="visualising-tabular-data.html#multiple-charts"><i class="fa fa-check"></i><b>2.5</b> Multiple charts</a></li>
<li class="chapter" data-level="2.6" data-path="visualising-tabular-data.html"><a href="visualising-tabular-data.html#overlay-graphic-elements"><i class="fa fa-check"></i><b>2.6</b> Overlay graphic elements</a></li>
<li class="chapter" data-level="2.7" data-path="visualising-tabular-data.html"><a href="visualising-tabular-data.html#parallel-coordinates"><i class="fa fa-check"></i><b>2.7</b> Parallel coordinates</a></li>
<li class="chapter" data-level="2.8" data-path="visualising-tabular-data.html"><a href="visualising-tabular-data.html#dimension-reduction"><i class="fa fa-check"></i><b>2.8</b> Dimension reduction</a><ul>
<li class="chapter" data-level="2.8.1" data-path="visualising-tabular-data.html"><a href="visualising-tabular-data.html#multidimensional-scaling-example"><i class="fa fa-check"></i><b>2.8.1</b> Multidimensional scaling example</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="visualising-tabular-data.html"><a href="visualising-tabular-data.html#summary"><i class="fa fa-check"></i><b>2.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis-of-trends-and-patterns.html"><a href="analysis-of-trends-and-patterns.html"><i class="fa fa-check"></i><b>3</b> Analysis of trends and patterns</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis-of-trends-and-patterns.html"><a href="analysis-of-trends-and-patterns.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="3.2" data-path="analysis-of-trends-and-patterns.html"><a href="analysis-of-trends-and-patterns.html#checking-for-normality"><i class="fa fa-check"></i><b>3.2</b> Checking for normality</a></li>
<li class="chapter" data-level="3.3" data-path="analysis-of-trends-and-patterns.html"><a href="analysis-of-trends-and-patterns.html#data-transformation"><i class="fa fa-check"></i><b>3.3</b> Data transformation</a></li>
<li class="chapter" data-level="3.4" data-path="analysis-of-trends-and-patterns.html"><a href="analysis-of-trends-and-patterns.html#other-kinds-of-curve-fitting"><i class="fa fa-check"></i><b>3.4</b> Other kinds of curve fitting</a></li>
<li class="chapter" data-level="3.5" data-path="analysis-of-trends-and-patterns.html"><a href="analysis-of-trends-and-patterns.html#uncertainty"><i class="fa fa-check"></i><b>3.5</b> Uncertainty</a></li>
<li class="chapter" data-level="3.6" data-path="analysis-of-trends-and-patterns.html"><a href="analysis-of-trends-and-patterns.html#clustering"><i class="fa fa-check"></i><b>3.6</b> Clustering</a></li>
<li class="chapter" data-level="3.7" data-path="analysis-of-trends-and-patterns.html"><a href="analysis-of-trends-and-patterns.html#hierarchical-clustering"><i class="fa fa-check"></i><b>3.7</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="3.8" data-path="analysis-of-trends-and-patterns.html"><a href="analysis-of-trends-and-patterns.html#summary-1"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html"><i class="fa fa-check"></i><b>4</b> Activity: Advanced plots with R</a><ul>
<li class="chapter" data-level="4.1" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html#introducing-ggplot2"><i class="fa fa-check"></i><b>4.1</b> Introducing ggplot2</a><ul>
<li class="chapter" data-level="4.1.1" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html#the-philosophy"><i class="fa fa-check"></i><b>4.1.1</b> The philosophy</a></li>
<li class="chapter" data-level="4.1.2" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html#layers"><i class="fa fa-check"></i><b>4.1.2</b> Layers</a></li>
<li class="chapter" data-level="4.1.3" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html#aesthetics"><i class="fa fa-check"></i><b>4.1.3</b> Aesthetics</a></li>
<li class="chapter" data-level="4.1.4" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html#geometric-objects-geoms"><i class="fa fa-check"></i><b>4.1.4</b> Geometric objects (geoms)</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html#visualisation-with-ggplot2"><i class="fa fa-check"></i><b>4.2</b> visualisation with ggplot2</a><ul>
<li class="chapter" data-level="" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html#step-1.-read-the-data"><i class="fa fa-check"></i>Step 1. read the data</a></li>
<li class="chapter" data-level="" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html#step-2.-first-graph"><i class="fa fa-check"></i>Step 2. first graph</a></li>
<li class="chapter" data-level="" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html#step-3.-change-the-shape"><i class="fa fa-check"></i>Step 3. change the shape</a></li>
<li class="chapter" data-level="" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html#step-4.-change-the-color"><i class="fa fa-check"></i>Step 4. change the color</a></li>
<li class="chapter" data-level="" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html#step-5.-facet"><i class="fa fa-check"></i>Step 5. Facet</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html#starting-at-the-end"><i class="fa fa-check"></i><b>4.3</b> Starting At The End</a></li>
<li class="chapter" data-level="4.4" data-path="activity-advanced-plots-with-r.html"><a href="activity-advanced-plots-with-r.html#now-continue-with-the-basic-examples-in-the-ggplot2-cheat-sheet"><i class="fa fa-check"></i><b>4.4</b> Now continue with the basic examples in the ggplot2 ‘cheat sheet’</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="activity-interactive-charts-with-r.html"><a href="activity-interactive-charts-with-r.html"><i class="fa fa-check"></i><b>5</b> Activity: Interactive charts with R</a><ul>
<li class="chapter" data-level="5.1" data-path="activity-interactive-charts-with-r.html"><a href="activity-interactive-charts-with-r.html#shiny"><i class="fa fa-check"></i><b>5.1</b> Shiny</a><ul>
<li class="chapter" data-level="5.1.1" data-path="activity-interactive-charts-with-r.html"><a href="activity-interactive-charts-with-r.html#structure-of-a-shiny-app"><i class="fa fa-check"></i><b>5.1.1</b> Structure of a Shiny App</a></li>
<li class="chapter" data-level="5.1.2" data-path="activity-interactive-charts-with-r.html"><a href="activity-interactive-charts-with-r.html#shiny-with-ggplot"><i class="fa fa-check"></i><b>5.1.2</b> Shiny with ggplot</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="activity-interactive-charts-with-r.html"><a href="activity-interactive-charts-with-r.html#other-ways"><i class="fa fa-check"></i><b>5.2</b> Other Ways</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="activity-clustering-with-r.html"><a href="activity-clustering-with-r.html"><i class="fa fa-check"></i><b>6</b> Activity: Clustering with R</a><ul>
<li class="chapter" data-level="6.1" data-path="activity-clustering-with-r.html"><a href="activity-clustering-with-r.html#clustering-irises-data"><i class="fa fa-check"></i><b>6.1</b> Clustering “irises” data</a></li>
<li class="chapter" data-level="6.2" data-path="activity-clustering-with-r.html"><a href="activity-clustering-with-r.html#clustering-crickets-data"><i class="fa fa-check"></i><b>6.2</b> Clustering “crickets” data</a></li>
<li class="chapter" data-level="6.3" data-path="activity-clustering-with-r.html"><a href="activity-clustering-with-r.html#hierarchical-clustering-1"><i class="fa fa-check"></i><b>6.3</b> Hierarchical clustering</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Module 2 Making sense of tabular data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="analysis-of-trends-and-patterns" class="section level1">
<h1><span class="header-section-number">3</span> Analysis of trends and patterns</h1>
<p>Although scatter plots, line graphs or bar charts can be helpful for exploring and understanding data, it can be difficult to tell what the overall trend or patterns are. Analytics can help this by summarising the data. Adding data summaries or <em>‘smoothers‘</em> can make it much, much easier to see the global patterns. Smoothing is an important way of exploring and understanding data. It can also be understood as a kind of model fitting. Another way of summarising data is to use clustering to group similar data items.</p>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">3.1</span> Simple linear regression</h2>
<p>The simplest way of smoothing data is simply to fit a straight line to it. This is called <em>linear regression</em>. Linear regression finds a line of ‘best fit’ that is as close as possible to all of the points.</p>
<p>We can also understand linear regression as a simple kind of model fitting. Given some data points <span class="math inline">\((x_{i},y_{i})\)</span> we want to find a linear model which allows us to predict the value of <span class="math inline">\(y_{i}\)</span> from <span class="math inline">\(x_{i}\)</span>. In other words <span class="math inline">\(x_{i}\)</span> is the independent variable and <span class="math inline">\(y_{i}\)</span> is the dependent variable we are trying to predict from <span class="math inline">\(x_{i}\)</span>.</p>
<p>Our linear model is the equation <span class="math inline">\(y_{i}=(a+bx_{i})+\epsilon_{i}\)</span> where a and b give the coefficients of the line of best fit: this is simply <span class="math inline">\(y=(a+bx)\)</span> where <span class="math inline">\(a\)</span> is the <span class="math inline">\(y\)</span> intercept and <span class="math inline">\(b\)</span> is the gradient of the line. The term equation <span class="math inline">\(ϵi\)</span> is called the residual: this is the difference between what the line predicts the value of <span class="math inline">\(y_{i}\)</span> will be and its actual value.</p>
<div class="figure">
<img src="diagrams_datasets/section3/Crickets.png" alt="An example of a regression line through some cricketing data. It shows the Indian cricket league (IPL) auction results in $1,000s for Australian players. If the line predicts correctly can 20 year old expect $1,000,000? A 55 year old nothing?" />
<p class="caption">An example of a regression line through some cricketing data. It shows the Indian cricket league (IPL) auction results in $1,000s for Australian players. If the line predicts correctly can 20 year old expect $1,000,000? A 55 year old nothing?</p>
</div>
<p>Linear regression finds the line (or more precisely the coefficients a and b) that best fits the data. This is found by minimising the sum of the squared residuals. Statisticians compute how well the line explains the data by computing something called <span class="math inline">\(R^2\)</span>. This is the proportion of the total variation that is explained by the model. A value near 1 is good, a value near 0 means the model explains very little.</p>
<p>When fitting data it is really important to plot the data and line of best fit in order to understand the residuals. Do not simply use a stats package to compute the line of best fit and <span class="math inline">\(R^2\)</span> and assume everything is fine if <span class="math inline">\(R^2\)</span> is not too close to 0. Remember Anscombe’s data quartet in which four very different data sets have the same line of best fit and same value of <span class="math inline">\(R^2\)</span>. Visual analysis is required to see that the line of best fit is appropriate in the top-left data set but not in the other three data sets. In the top right the relationship is clearly not linear and a non-liner model is required while in the bottom two data sets a single outlier has skewed the line of best fit.</p>
<p><img src="diagrams_datasets/section3/Anscombe.png" /></p>
<p>In fact it is a good idea to plot the residuals themselves as this really allows you to understand what the model does <strong>not</strong> explain. Looking at the residuals allows you to see what other things you can add to the model to better fit the data.</p>
<p><img src="diagrams_datasets/section3/AnscombeResiduals.png" /></p>
</div>
<div id="checking-for-normality" class="section level2">
<h2><span class="header-section-number">3.2</span> Checking for normality</h2>
<p>Another reason for graphing the residuals is that it allows you to check an assumption that underlies regression and correlation statistics: that the residuals are random normally distributed variables with mean of 0. Note it is only the residuals that need to be normally distributed, not the original values.</p>
<p>There are two ways of plotting data to check that it is normally distributed. The first is to look at a histogram or density plot of the data distribution and see if it looks like a normal distribution centered around the mean.</p>
<p>Lets look at our cricketing auction results. If we plot the histogram for the residuals then this looks like normal distribution.</p>
<p><img src="diagrams_datasets/section3/resid-290x300.png" /></p>
<p>Another way of testing for normality is to use a <em>Q-Q plot</em>. A Q-Q plot compares two probability distributions. It plots the quantiles of one distribution against those of the other distribution. Hence the name, Q-Q stands for quantile-quantile plot. If the points in the plot lie along the <span class="math inline">\(y=x\)</span> line then the two probability distributions are the same. If they lie along a straight line then they are the same except for scaling and translation.</p>
<p>To test for normality you simply plot the data distribution quantiles against those of the standard distribution. If the quartiles lie on a reasonably straight line then the data distribution is normal.</p>
<p>If we look at the Q-Q plot for the residuals from the cricketing example we see they fall reasonably well along a straight line so we can be reasonably confident the residuals are normally distributed.</p>
<p><img src="diagrams_datasets/section3/QQ-300x288.png" /></p>
<p>Interpreting a QQ plot</p>
<ul>
<li>Perfectly normally distributed residuals will align along the identity (y=x) line.</li>
<li>Short tails will veer of the line horizontally.</li>
<li>Long tails will veer off the line vertically.</li>
<li><em>Expect some variation, even from normal residuals!</em></li>
</ul>
</div>
<div id="data-transformation" class="section level2">
<h2><span class="header-section-number">3.3</span> Data transformation</h2>
<p>If the data (such as the residuals) are not normally distributed then there are a number of ways to handle this. One way is to use robust or non-parametric tests which do not rely on assumptions of normality. These typically rely on trimming the data to reduce the effect of outliers or simply using the relative rank of the values, not their precise amount.</p>
<p>Another way is to uniformly transform the observations to remove kurtosis or skew. Some useful transformations (from Field et al, <em>Discovering Statistics Using R</em>, 2012) are</p>
<ul>
<li><em>Log transformation</em> (<span class="math inline">\(log(X)\)</span>). This squashes the right tail of the distribution and can correct for positive skew and unequal variances between distributions. Be careful though as it only makes sense for positive numbers so you may need to add a constant to the data first.</li>
<li><em>Square root transformation</em> (<span class="math inline">\(sqrt(X)\)</span>). This has a similar effect to the log transformation but is not so severe. Again the data must be positive.</li>
<li><em>Reciprocal transformation</em> (<span class="math inline">\(1/X\)</span>). This also reduces the impact of large scores. Be careful because it reverses the ranking of scores to fix this you can use <span class="math inline">\(1/(X_{max}-X)\)</span></li>
<li><em>Reverse score transformation</em>: To overcome the effects of negative skewness you can simply reverse the scores by for instance applying the transformation <span class="math inline">\((X_{max}-X)\)</span> and then applying one of the above transformations.</li>
</ul>
</div>
<div id="other-kinds-of-curve-fitting" class="section level2">
<h2><span class="header-section-number">3.4</span> Other kinds of curve fitting</h2>
<p>In linear regression we fit a single straight line to the data using one variable as a predictor. There are, of course, many more complex curves and surfaces that we can fit to the dependent variables, all of which are a kind of model that allows us to predict the value of the dependent variables from the independent variables. Some of the more common include</p>
<ul>
<li><em>Multiple (linear)</em> regression: in which the dependent data is explained by more than one independent variable. In this case we still fit a line to the data but it has a gradient coefficient for each of the independent variables.</li>
<li><em>Polynomial regression</em>: we can fit more complex polynomials to the data.</li>
<li><em>LOESS</em>: locally weighted polynomial regression. This fits a low-degree polynomial to a subset of the data around each data point.</li>
</ul>
<p>Regardless of the type of curve fitted it is always useful to plot the residuals so as to better understand what the model does and does not explain.</p>
<p>So far we have assumed that the values of the independent values are perfectly known, or in fact that we have independent and dependent variables. If we want to treat both kinds of variables equivalently then we must use <em>total least squares</em>. In total least squares the residual measures the distance between the data point and the closest point on the fitted curve.</p>
</div>
<div id="uncertainty" class="section level2">
<h2><span class="header-section-number">3.5</span> Uncertainty</h2>
<p>There is however a potential problem with simple visualisation of the line of best fit: it is difficult to understand the level of uncertainty in its predictive power. Given that the residuals are normally distributed it is possible to compute the region around the line of best fit in which the actual data values will lie with probability of say 80 or 90%.</p>
<p>When you can, it is important in data visualisation to visually indicate uncertainty so that the reader gets a sense of how much they can trust the data or model. If this is not done it is easy to believe that the visualisation is completely accurate: seeing is believing.</p>
<p><img src="diagrams_datasets/section3/AnscombeConf.png" /></p>
</div>
<div id="clustering" class="section level2">
<h2><span class="header-section-number">3.6</span> Clustering</h2>
<p>Another way of summarising data or seeing patterns is to use clustering. Clustering tries to group “similar” data items together. It is core part of data science and is covered in more detail in <a href="">Dependence, regression and clustering</a>. There are a large number of approaches to clustering, see for instance <a href="https://en.wikipedia.org/wiki/Cluster_analysis">Cluster Analysis (wikipedia)</a>. Clustering techniques can be categorised into</p>
<ul>
<li><em>hard</em> or <em>soft clustering</em>: in hard clustering an item either belongs to a cluster or it doesn’t while in soft or fuzzy clustering an item has some likelihood of belonging to a cluster</li>
<li><em>strict</em> or <em>overlapping partitioning</em>: in strict partitioning an item belongs to exactly one cluster, while in overlapping partitioning it can belong to more than one</li>
<li><em>hierarchical clustering</em>: the clusters form a hierarchy in the sense that an item that belongs to a child cluster also belongs to the parent cluster.</li>
</ul>
<p>There is no single best clustering algorithm and all of them have limitations. This means it is very important to visualise the results and see if the clustering is sensible, or to compare different clusterings. For instance in the following example taken from wikimedia, we see that k-means clustering gives quite a different result to EM clustering.</p>
<p><img src="diagrams_datasets/section3/mouse-clustering.png" /></p>
<p><a href="https://en.wikipedia.org/wiki/K-means_clustering">K-means Clustering</a> iteratively computes the centroid of each cluster while the <a href="https://en.wikipedia.org/wiki/Expectation–maximization_algorithm">EM (Expectation-Maximization) Algorithm</a> iteratively refines the unknown parameters of a Gaussian distribution model for the clusters and at each step computing the likelihood each item belongs to a particular cluster. In this case the EM algorithm performs better, giving a result closer to the known clustering shown on the left.</p>
<p>The basic steps in clustering analysis are</p>
<ol style="list-style-type: decimal">
<li>Normalise the data if necessary. The most common approach is to use <a href="https://en.wikipedia.org/wiki/Standard_score">z-scores</a>.</li>
<li>Decide how to measure similarity between items. In the example above we used Euclidean distance but there are many other choices.</li>
<li>Choose the clustering method and, if required, the number of clusters.</li>
<li>Visualise the clusters and try to determine if they are meaningful and if so what they mean.</li>
</ol>
</div>
<div id="hierarchical-clustering" class="section level2">
<h2><span class="header-section-number">3.7</span> Hierarchical clustering</h2>
<p><em>Connectivity based clustering</em> is an intuitively simple and commonly used class of methods for clustering. These create a hierarchical cluster by iteratively joining the two most similar clusters to form a new parent cluster. Initially each item forms its own cluster. As an example consider the UPGMA (unweighted pair-group method using arithmetic averages) method applied to a hypothetical data set of 5 items with two attributes.</p>
<div class="figure">
<img src="diagrams_datasets/section3/thematic-cartography-geovis-fig.18.18-ab-960x403.png" alt="Left: a generated raw data set; right: a plot of each observation (Based on figure 18.18ab from Thematic Cartography and Geovisualization 3rd Ed by Slocum, T. A. and etc.)" />
<p class="caption">Left: a generated raw data set; right: a plot of each observation (Based on figure 18.18ab from Thematic Cartography and Geovisualization 3rd Ed by Slocum, T. A. and etc.)</p>
</div>
<p>Initially each observation is assigned to its own cluster. To measure similarity between two clusters we use the average Euclidean distance between members of the two clusters.</p>
<div class="figure">
<img src="diagrams_datasets/section3/thematic-cartography-geovis-fig.18.18-c-300x269.png" alt="The resemblance matrix of Euclidean distance (Based on figure 18.18c from Thematic Cartography and Geovisualization 3rd Ed by Slocum, T. A. and etc.)" />
<p class="caption">The resemblance matrix of Euclidean distance (Based on figure 18.18c from Thematic Cartography and Geovisualization 3rd Ed by Slocum, T. A. and etc.)</p>
</div>
<p>The two clusters with the least average distance between them are merged to form a new cluster. In this case the observations 1 and 2 are merged to form the new cluster 1-2.</p>
<div class="figure">
<img src="diagrams_datasets/section3/thematic-cartography-geovis-fig.18.18-d-300x216.png" alt="Combine observation 1 and 2 because they have the smallest Euclidean distance (Based on figure 18.18d from Thematic Cartography and Geovisualization 3rd by Slocum, T. A. and etc.)" />
<p class="caption">Combine observation 1 and 2 because they have the smallest Euclidean distance (Based on figure 18.18d from Thematic Cartography and Geovisualization 3rd by Slocum, T. A. and etc.)</p>
</div>
<p>We repeat this process: next observations 4 and 5 are merged</p>
<div class="figure">
<img src="diagrams_datasets/section3/thematic-cartography-geovis-fig.18.18-ef-960x432.png" alt="Combine observation 4 and 5 because they have the smallest Euclidean distance (Based on figure 18.18ef from Thematic Cartography and Geovisualization 3rd by Slocum, T. A. and etc.)" />
<p class="caption">Combine observation 4 and 5 because they have the smallest Euclidean distance (Based on figure 18.18ef from Thematic Cartography and Geovisualization 3rd by Slocum, T. A. and etc.)</p>
</div>
<p>then 3 is merged with cluster 1-2</p>
<div class="figure">
<img src="diagrams_datasets/section3/thematic-cartography-geovis-fig.18.18-gh-960x438.png" alt="Combine cluster(1-2) and observation 3 because they have the smallest Euclidean distance (Based on figure 18.18gh from Thematic Cartography and Geovisualization 3rd by Slocum, T. A. and etc.)" />
<p class="caption">Combine cluster(1-2) and observation 3 because they have the smallest Euclidean distance (Based on figure 18.18gh from Thematic Cartography and Geovisualization 3rd by Slocum, T. A. and etc.)</p>
</div>
<p>and now the final two clusters are merged.</p>
<p>The result of hierarchical clustering is usually shown using a dendrogram. In this graphic a tree shows the cluster hierarchy with the position of the parent node showing how similar the two clusters are. In our example for instance it shows that clusters 1 and 2 combined with an average Euclidean distance of 5 while clusters 3 and 1-2 combined with an average distance of 12.35.</p>
<div class="figure">
<img src="diagrams_datasets/section3/thematic-cartography-geovis-fig.18.19-960x530.png" alt="Dendrogram for the generated data (Based on figure 18.19 from Thematic Cartography and Geovisualization 3rd by Slocum, T. A. and etc.)" />
<p class="caption">Dendrogram for the generated data (Based on figure 18.19 from Thematic Cartography and Geovisualization 3rd by Slocum, T. A. and etc.)</p>
</div>
<p>The <a href="https://en.wikipedia.org/wiki/Cophenetic_correlation"><em>cophenetic correlation coefficient</em></a> can be used to check how well the hierarchical clustering explains the data. This measures the correlation between the raw similarity between each pair of items and the similarity given in the dendrogram. For instance, in our example the raw similarity between 1 and 3 is 12.04 but the similarity in the dendrogram is 12.35 since this is the average Euclidean distance associated with the cluster 1-2-3 which is the smallest cluster containing both 1 and 3. More sophisticated analyses are also possible.</p>
</div>
<div id="summary-1" class="section level2">
<h2><span class="header-section-number">3.8</span> Summary</h2>
<p>Curve-fitting and clustering are two commonly used statistical techniques for summarising tabular data. Curve fitting smooths the data and can reveal trends while clustering groups similar items. In both cases it is important to “sanity check” the results using visualisation. It is also important to try different “smoothers” and clustering methods as there is no one best method and different methods can give quite different results.</p>
<p><br><br></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="visualising-tabular-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="activity-advanced-plots-with-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "Module"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
